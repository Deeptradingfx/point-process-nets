@article{elman1990srnn,
	author = {Elman, Jeffrey L.},
	title = {Finding Structure in Time},
	journal = {Cognitive Science},
	volume = {14},
	number = {2},
	pages = {179-211},
	year = {1990},
	doi = {10.1207/s15516709cog1402\_1},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402_1},
	abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.}
}

@inproceedings{paszke2017automatic,
	title={Automatic differentiation in PyTorch},
	author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	booktitle={NIPS-W},
	year={2017}
}

@article{ogata1981,
	author = {Ogata, Yosihiko},
	year = {1981},
	month = {01},
	pages = {23-30},
	title = {On Lewis' Simulation Method for Point Processes},
	volume = {27},
	journal = {IEEE Transactions on Information Theory},
	doi = {10.1109/TIT.1981.1056305}
}

@article{2015arXiv150204592B,
	author = {{Bacry}, E. and {Mastromatteo}, I. and {Muzy}, J.-F.},
	title = "{Hawkes processes in finance}",
	journal = {ArXiv e-prints},
	archivePrefix = "arXiv",
	eprint = {1502.04592},
	primaryClass = "q-fin.TR",
	keywords = {Quantitative Finance - Trading and Market Microstructure},
	year = 2015,
	month = feb,
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150204592B},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2017arXiv170703003B,
	author = {{Bacry}, E. and {Bompaire}, M. and {Ga{\"i}ffas}, S. and {Poulsen}, S.},
  	title = "{tick: a Python library for statistical learning, with
    a particular emphasis on time-dependent modeling}",
  	journal = {ArXiv e-prints},
  	eprint = {1707.03003},
  	year = 2017,
  	month = jul
}

@article{bowsherModellingMktEvents2002,
	author = {G. Bowsher, Clive},
	year = {2002},
	month = {11},
	pages = {},
	title = {Modelling Security Market Events in Continuous Time: Intensity Based, Multivariate Point Process Models},
	volume = {141},
	journal = {SSRN Electronic Journal},
	doi = {10.2139/ssrn.343020}
}

@misc{unreasonableEffectivenessRNN,
	author = {Andrej Kaparthy},
	title = {The Unreasonable Effectiveness of Recurrent Neural Networks},
	date = {2015-05-21},
	url = {http://karpathy.github.io/2015/05/21/rnn-effectiveness/},
}

@article{meiEisnerNeuralHawkes,
	author    = {Hongyuan Mei and
	Jason Eisner},
	title     = {The Neural Hawkes Process: {A} Neurally Self-Modulating Multivariate
	Point Process},
	journal   = {CoRR},
	volume    = {abs/1612.09328},
	year      = {2016},
	url       = {http://arxiv.org/abs/1612.09328},
	archivePrefix = {arXiv},
	eprint    = {1612.09328},
	timestamp = {Mon, 13 Aug 2018 16:48:23 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/MeiE16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

 Leave an email address: or
or[Not interested]
ACM DL 	
Ecole Polytechnique


SIGN IN   SIGN UP

Recurrent Marked Temporal Point Processes: Embedding Event History to Vector
Full Text: 	PDFPDF
Authors: 	Nan Du 	Georgia Institute of Technology, Atlanta, GA, USA
Hanjun Dai 	Georgia Institute of Technology, Atlanta, GA, USA
Rakshit Trivedi 	Georgia Institute of Technology, Atlanta, GA, USA
Utkarsh Upadhyay 	Max Planck Institute for Software Systems, Kaiserslautern, Germany
Manuel Gomez-Rodriguez 	Max Planck Institute for Software Systems, Kaiserslautern, Germany
Le Song 	Georgia Institute of Technology, Atlanta, GA, USA

Recurrent Marked Temporal Point Processes: Embedding Event History to Vector 	Published by ACM 2016 Article

Poster

Bibliometrics Data  Bibliometrics
· Citation Count: 12
· Downloads (cumulative): 550
· Downloads (12 Months): 221
· Downloads (6 Weeks): 36

Published in:
Cover Image

· Proceeding
KDD '16 Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
Pages 1555-1564

San Francisco, California, USA — August 13 - 17, 2016
ACM New York, NY, USA ©2016
table of contents ISBN: 978-1-4503-4232-2 doi>10.1145/2939672.2939875


Tools and Resources

Request Permissions
TOC Service: Spacer Image reserves space for checkmark when TOC Service is updated

Toc Alert via EmailEmail Toc Alert via EmailRSS 
Save to Binder
Export Formats:
BibTeX EndNote ACM Ref 
Upcoming Conference:
KDD '19 

Share:
|
Author Tags Expand Author Tags
Contact The DL Team Contact Us | Switch to single page view (no tabs)

Large volumes of event data are becoming increasingly available in a wide variety of applications, such as healthcare analytics, smart cities and social network analysis. The precise time interval or the exact distance between two events carries a great deal of information about the dynamics of the underlying systems. These characteristics make such data fundamentally different from independently and identically distributed data and time-series data where time and space are treated as indexes rather than random variables. Marked temporal point processes are the mathematical framework for modeling event data with covariates. However, typical point process models often make strong assumptions about the generative processes of the event data, which may or may not reflect the reality, and the specifically fixed parametric assumptions also have restricted the expressive power of the respective processes. Can we obtain a more expressive model of marked temporal point processes? How can we learn such a model from massive data?

In this paper, we propose the Recurrent Marked Temporal Point Process (RMTPP) to simultaneously model the event timings and the markers. The key idea of our approach is to view the intensity function of a temporal point process as a nonlinear function of the history, and use a recurrent neural network to automatically learn a representation of influences from the event history. We develop an efficient stochastic gradient algorithm for learning the model parameters which can readily scale up to millions of events. Using both synthetic and real world datasets, we show that, in the case where the true models have parametric specifications, RMTPP can learn the dynamics of such models without the need to know the actual parametric forms; and in the case where the true models are unknown, RMTPP can also learn the dynamics and achieve better predictive performance than other parametric alternatives based on particular prior assumptions.

Powered by The ACM Guide to Computing Literature

The ACM Digital Library is published by the Association for Computing Machinery. Copyright © 2018 ACM, Inc.
Terms of Usage   Privacy Policy   Code of Ethics   Contact Us
BibTeX | EndNote | ACM Ref

@inproceedings{DuRMTPP,
	author = {Du, Nan and Dai, Hanjun and Trivedi, Rakshit and Upadhyay, Utkarsh and Gomez-Rodriguez, Manuel and Song, Le},
	title = {Recurrent Marked Temporal Point Processes: Embedding Event History to Vector},
	booktitle = {Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	series = {KDD '16},
	year = {2016},
	isbn = {978-1-4503-4232-2},
	location = {San Francisco, California, USA},
	pages = {1555--1564},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/2939672.2939875},
	doi = {10.1145/2939672.2939875},
	acmid = {2939875},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {marked temporal point process, recurrent neural network, stochastic process},
} 

