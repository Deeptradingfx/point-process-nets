% !TeX spellcheck = fr_FR
\documentclass[11pt]{article}
\usepackage[a4paper,hmargin=3.6cm]{geometry}
\usepackage{subfiles}
\usepackage{polyglossia}
\setdefaultlanguage{french}

\usepackage{mathtools}
\usepackage{amsfonts,amssymb}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage[
	page,title
]{appendix}

\usepackage{dsfont}

\usepackage{csquotes}
\usepackage[backend=biber,sorting=nyt]{biblatex}

\usepackage{hyperref}

\title{\textbf{Processus ponctuels récurrents}\\
	\textit{Rapport de projet}  
}
\author{
  Wilson \textsc{Jallet}\\
  Cheikh \textsc{Fall}\\
  \textit{sous la supervision d'Emmanuel BACRY}
}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}

% \newcommand*{\Appendixautorefname}{annexe}
\renewcommand{\appendixpagename}{Annexes} 

\bibliography{references.bib}

\begin{document}
\maketitle

\section{Introduction}

On cherche à modéliser des flux d'événements discrets, arrivant à des instants $t_i$ et associés à des \textit{types} $k_i$, et dont l'évolution dépend du passé: des événements de certains types peut en inciter ou \textit{exciter} d'autres (dans le sens où leur arrivée devient plus probable dans le futur). Par exemple, ces événements peuvent être des messages sur un réseau social comme Twitter (le type pourrait alors être la <<~popularité~>> de l'auteur du message, et chaque message en inciterait d'autres sous la forme de \textit{retweets}), ou des ordres sur un marché financier, où les types seraient s'il s'agit d'un achat (\textit{bid}), d'une vente (\textit{ask}), d'une annulation (\textit{cancel}), au prix du marché ou exécuté à partir d'un certain prix.

Une classe de modèles statistiques utiles pour modéliser l'arrivée d'événements discrets est celle des \textit{processus ponctuels}. Un modèle usuel pour modéliser des flux d'événements avec dépendance du futur sur le passé est celui de Hawkes, où les intensités sont des fonctions assez simples des événements passés. On se propose ici d'explorer des modèles inspirés de Hawkes, s'appuyant sur des réseaux de neurones récurrents permettant d'obtenir des structures de dépendance plus riches, et les appliquer à la prédiction des natures et temps d'arrivée d'événements futurs.

\section{Modèles}

\subfile{parts/models}


\section{Algorithmes et implémentation}

\subfile{parts/algos}


\printbibliography

\begin{appendices}
\section{Algorithmes}\label{sec:algoAppendix}

\subsection{Calcul de la log-vraisemblance}

La log-vraisemblance donnée par la relation \eqref{eq:explicitLikelihood} comporte deux termes: une somme de logarithmes d'intensités et l'intégrale $\int_0^T\lambda_t\,dt$ de l'intensité totale du processus (la somme des intensités de chaque type).

\paragraph{Estimation de l'intégrale.} Étant donné un variable aléatoire $\tau\sim\mathcal{U}([0,T])$ indépendante de la filtration $\mathds{F}$, la quantité 
\[
\bar{\Lambda} = T\lambda_\tau\cdot\mathbf 1
\]
est un estimateur non biaisé de l'intégrale. Mais il pose un problème: il faut savoir à quel intervalle $(t_{i-1}, t_i]$ appartient $\tau$, étant donné que l'expression de $\lambda_t$ est définie par morceaux. En utilisant la relation de Chasles, on peut construire l'estimateur (non biaisé) suivant:
\begin{equation}
\hat{\Lambda}^C = \sum_{i=1}^{I} \Delta t_i\lambda_{\tau_i} \cdot \mathbf{1} + (T-t_{I})\lambda_{\tau_{I+1}}\cdot\mathbf{1}
\approx
\int_0^T \lambda_t\,dt
\end{equation}
où les $\tau_i\sim\mathcal{U}([t_{i-1}, t_i])$, $\tau_{I+1}\in\sim\mathcal{U}([t_I, T])$ sont indépendants. Le problème de l'intervalle auquel appartient $\tau$ est levé, mais on n'a aucune garantie que $\hat{\Lambda}^C$ soit un meilleur ou pire estimateur que $\bar{\Lambda}$ en terme de variance.

\paragraph{Somme des logarithmes.} Le premier terme pose un léger problème au niveau de l'implémentation. Le calcul des intensités ${(\lambda_{t_i})}_i$ donne un tenseur \verb|intensities_at_events| de format $I\times B\times K$, avec $B$ la taille du \textit{batch} (le sous-ensemble d'entraînement que l'on est en train de traiter). Extraire les $\lambda^{k_i}_{t_i}$ ne peut pas se faire par indexation (par exemple en faisant \verb|intensity_at_events[:, :, event_types]|).

L'astuce utilisée exploite la représentation des types $k_i$ par encodage \textit{one-hot} avec des vecteurs $x_i\in{\{0,1\}}^K$. On a la relation suivante:
\[
	\lambda^{k_i}_{t_i} = \sum_{k=1}^K {(\lambda_{\tau_i} \odot x_i)}_k,
\]
où $\odot$ désigne le produit terme-à-terme, qui permet d'obtenir les composantes de l'intensité que l'on veut en faisant un produit et une réduction.

\end{appendices}
\end{document}