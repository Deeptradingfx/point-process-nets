% !TeX spellcheck = fr_FR
\documentclass[11pt]{article}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{titlesec}

\usepackage{polyglossia}
\setdefaultlanguage{french}

\title{\textbf{Rapport de projet}\\
  \textit{Processus ponctuels récurrents}
}
\author{
  Wilson \textsc{Jallet}\\
  Cheikh \textsc{Fall}
}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}


\begin{document}
\maketitle

\section{Introduction}


\section{Modèles}

Un facteur important dans la prédiction d'événements est si le modèle utilisé peut prendre en compte les événements passés.

\subsection{Modèle de base: le processus de Hawkes}

Un processus de Hawkes est un processus ponctuel $\{(t_i, k_i)\}$ modélisant l'arrivée d'événements de types $k_i\in[1..K]$ à des instants $t_i$. C'est le modèle élémentaire de processus dont l'intensité $\lambda_t$ dépend du passé du processus. Sa mesure d'intensité conditionnellement au passé du processus s'écrit souvent sous la forme (vectorielle)
\begin{equation}
	\lambda_t = \mu_t + \int_0^t g(t-s)\,dN_s
\end{equation}
où:\begin{itemize}
	\item $N^i_t$ est le nombre total d'événements de type $i$,
	\item $\lambda_t^i$ est l'intensité des événements de type $i$,
	\item $\mu^i_t$ est l'intensité de base des événements de type $i$,
	\item $g(t) \in \RR^{K\times K}$ et le \textit{noyau} d'excitation: le coefficient $g_{ij} \geq 0$ contrôle le degré selon lequel les événements de type $j$ influencent l'arrivée des événements de type $j$.
\end{itemize} 

\subsection{Réseaux de neurones récurrents}

Le principe d'un réseau de neurone récurrent (\textit{recurrent neural network}, RNN) est que l'entraînement du réseau prend en compte les événements passés, via un état caché $h$ (\textit{hidden state}), mis à jour à chaque passage dans le réseau.

\subsection{Réseau récurrent avec amortissement (Decay-RNN)}

Il s'agit d'une variante du réseau récurrent simple.

On postule comme intensité
\begin{equation}\label{eq:decayrnnhiddenstate}
	\lambda_t = f(W_l h(t))
\end{equation}
où l'état caché $h$ est rendu continu, prenant en compte le temps écoulé entre chaque événement par un amortissement
\begin{equation}
	h(t) = h_{i-1}e^{-\delta_i(t-t_{i-1})},\quad t\in(t_{i-1},t_i]
\end{equation}
où les paramètres $h$ et $\delta$ sont mis à jour pour l'intervalle $(t_i,t_{i+1}]$ via les équations
\begin{align}
	h_{i+1} &= \tanh(W_{hh}h(t_i) + b_{hh} + W_{ih}k_i + b_{ih}) \\
	\delta_{i+1} &= f(W_dh(t_i) + b_d)
\end{align}
On utilise donc le fait que $h(t_i) = h_{i-1}\exp(-\delta_i\Delta t_i)$ d'après \eqref{eq:decayrnnhiddenstate}.

\end{document}