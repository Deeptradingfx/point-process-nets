{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x113be6350>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(3456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden Layer: $h_j = \\max{(W^{y}y_{j} + W^{t}t_{j} + W^{h}h_{j-1} + b_{h},0)} $\n",
    "\n",
    "\n",
    "#### Marker Generation: $P(y_{j+1}=k\\mid h_{j}) = \\frac{\\exp(V_{k,:}^{y}h_{j} + b_{k}^{y})}{\\sum_{k=1}^{K} \\exp(V_{k,:}^{y}h_{j} + b_{k}^{y})} = \\sigma(z)_{k}$ where $\\sigma$ is softmax function and $z$ is the vector $ V^{y}h_{j} + b^{y}$ \n",
    "\n",
    "\n",
    "#### Conditional Density: $f^{*}(t) = \\exp\\{{v^{t}}^\\top.h_{j} + w^t(t-t_{j}) + b^{t} + \\frac{1}{w}\\exp({v^{t}}^\\top.h_{j} + b^{t}) -\\frac{1}{w}\\exp({v^{t}}^\\top.h_{j} + w^t(t-t_{j}) + b^{t} )\\} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rmtpp(nn.Module):\n",
    "    \n",
    "    def __init__(self,marker_dim):\n",
    "        self.N = 1000\n",
    "        #marker_dim equals to time_dim\n",
    "        super(Rmtpp, self).__init__()\n",
    "       \n",
    "        #linear transformation\n",
    "        self.lin_op = nn.Linear(3,1) \n",
    "        self.vt = nn.Linear(1,1)\n",
    "        \n",
    "        #weights\n",
    "        self.w_t = torch.rand(1)\n",
    "        self.w = torch.rand(1)\n",
    "        self.V_y = torch.rand(marker_dim) #marker dim = number of markers \n",
    "        self.b_y = torch.rand(marker_dim) #bias\n",
    "        \n",
    "    #compute integral of t*fstart(t) between tj and +infinity using monteCarlo method\n",
    "    def numerical_mean(self,tj,hj):  \n",
    "        N = self.N\n",
    "        samp = torch.randn(N,1)\n",
    "        samp = (samp>tj).float() * samp\n",
    "        #print(samp)\n",
    "        sample = torch.sqrt(torch.tensor([2*F.math.pi]*N).unsqueeze(-1)) * torch.exp(1/2 * samp**2) * samp * self.fstart(samp,tj,hj).unsqueeze(-1) \n",
    "        sample = sample.squeeze(1)\n",
    "        return torch.tensor([torch.mean(sample)]).unsqueeze(-1)\n",
    "    \n",
    "    #compute integral of lambda*(t) between tj and t using trapezoidal method\n",
    "    def numerical_integration(self,tj,t,hj):\n",
    "        N = self.N\n",
    "        n = t.size()[0]\n",
    "        delta_t = (t - tj)/N\n",
    "        dt = delta_t.repeat(1,N)\n",
    "        c_ = torch.reshape(tj.repeat(n),(n,1)) \n",
    "        dt = torch.cat((c_,dt),dim=1)\n",
    "        \n",
    "        dt = torch.cumsum(dt,dim=1)\n",
    "        lam = self.lambda_star(dt,tj,hj)\n",
    "        sum_ = delta_t * (lam[:,1:] + lam[:,:-1]) /2\n",
    "       \n",
    "        return torch.sum(sum_,dim=1)\n",
    "        \n",
    "        \n",
    "    #compute the function lambdastar  \n",
    "    def lambda_star(self,t,tj,hj):\n",
    "        return torch.exp(self.vt(hj) - torch.exp(self.w_t)*(t-tj))\n",
    "    \n",
    "    #compute the function fstar\n",
    "    def fstart(self,t,tj,hj):\n",
    "        return self.lambda_star(t,tj,hj).squeeze(1) * torch.exp(-1*self.numerical_integration(tj,t,hj))\n",
    "    \n",
    "          \n",
    "        \n",
    "    def forward(self, time, marker, hidden_state):\n",
    "        #I first compute next time\n",
    "        tj = time\n",
    "        time = self.numerical_mean(time,hidden_state)\n",
    "        print(\"output time\",time)\n",
    "        logfstar = -1 * torch.log(self.fstart(time,tj,hidden_state))\n",
    "    \n",
    "        #Then next marker distribution\n",
    "        soft_max = nn.LogSoftmax(dim=0) #softmax of rows\n",
    "        logprob = -1 * soft_max(self.V_y*hidden_state + self.b_y) \n",
    "        \n",
    "        #Finally next hidden_state\n",
    "        input_ = torch.cat((marker, time.squeeze(1), hidden_state))\n",
    "        print(\"input_cat\",input_)\n",
    "        print(self.lin_op(input_))\n",
    "        hidden_state = F.relu(self.lin_op(input_))\n",
    "        \n",
    "        return logprob, logfstar, hidden_state\n",
    "    \n",
    "    \n",
    "    def log_likelihood(self,log_time_series_list,log_marker_series_list):\n",
    "        #time series and marker series are output of the RMTPP network\n",
    "        #return log_likelihood of all sequences\n",
    "        lll = 0\n",
    "        for i in range(len(log_time_series_list)):\n",
    "            lll += torch.sum(log_time_series_list) + torch.sum(log_marker_series_list)\n",
    "        return lll       \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "input time tensor([0.1454])\n",
      "output time tensor([[0.6022]])\n",
      "input_cat tensor([0.0000, 0.6022, 0.0000])\n",
      "tensor([-0.7841], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "1\n",
      "input time tensor([0.1842])\n",
      "output time tensor([[0.5614]])\n",
      "input_cat tensor([1.0000, 0.5614, 0.0000], grad_fn=<CatBackward>)\n",
      "tensor([-0.9384], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "2\n",
      "input time tensor([0.5911])\n",
      "output time tensor([[0.7510]])\n",
      "input_cat tensor([1.0000, 0.7510, 0.0000], grad_fn=<CatBackward>)\n",
      "tensor([-1.0380], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "3\n",
      "input time tensor([1.5678])\n",
      "output time tensor([[1.5228]])\n",
      "input_cat tensor([0.0000, 1.5228, 0.0000], grad_fn=<CatBackward>)\n",
      "tensor([-1.2680], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "4\n",
      "input time tensor([1.5824])\n",
      "output time tensor([[1.6864]])\n",
      "input_cat tensor([0.0000, 1.6864, 0.0000], grad_fn=<CatBackward>)\n",
      "tensor([-1.3540], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "5\n",
      "input time tensor([1.9706])\n",
      "output time tensor([[1.0092]])\n",
      "input_cat tensor([0.0000, 1.0092, 0.0000], grad_fn=<CatBackward>)\n",
      "tensor([-0.9980], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "6\n",
      "input time tensor([2.9309])\n",
      "output time tensor([[1.1891]])\n",
      "input_cat tensor([1.0000, 1.1891, 0.0000], grad_fn=<CatBackward>)\n",
      "tensor([-1.2683], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "7\n",
      "input time tensor([3.6977])\n",
      "output time tensor([[nan]])\n",
      "input_cat tensor([0.0000,    nan, 0.0000], grad_fn=<CatBackward>)\n",
      "tensor([nan], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "8\n",
      "input time tensor([3.8282])\n",
      "output time tensor([[nan]])\n",
      "input_cat tensor([1.0000,    nan,    nan], grad_fn=<CatBackward>)\n",
      "tensor([nan], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "9\n",
      "input time tensor([3.8690])\n",
      "output time tensor([[nan]])\n",
      "input_cat tensor([1.0000,    nan,    nan], grad_fn=<CatBackward>)\n",
      "tensor([nan], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "10\n",
      "input time tensor([3.9925])\n",
      "output time tensor([[nan]])\n",
      "input_cat tensor([0.0000,    nan,    nan], grad_fn=<CatBackward>)\n",
      "tensor([nan], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "11\n",
      "input time tensor([4.1099])\n",
      "output time tensor([[nan]])\n",
      "input_cat tensor([1.0000,    nan,    nan], grad_fn=<CatBackward>)\n",
      "tensor([nan], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "12\n",
      "input time tensor([4.2349])\n",
      "output time tensor([[nan]])\n",
      "input_cat tensor([0.0000,    nan,    nan], grad_fn=<CatBackward>)\n",
      "tensor([nan], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "13\n",
      "input time tensor([4.5288])\n",
      "output time tensor([[nan]])\n",
      "input_cat tensor([1.0000,    nan,    nan], grad_fn=<CatBackward>)\n",
      "tensor([nan], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "14\n",
      "input time tensor([4.8825])\n",
      "output time tensor([[nan]])\n",
      "input_cat tensor([0.0000,    nan,    nan], grad_fn=<CatBackward>)\n",
      "tensor([nan], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "15\n",
      "input time tensor([4.9065])\n",
      "output time tensor([[nan]])\n",
      "input_cat tensor([0.0000,    nan,    nan], grad_fn=<CatBackward>)\n",
      "tensor([nan], grad_fn=<ThAddBackward>)\n",
      "----------\n",
      "16\n",
      "input time tensor([4.9735])\n",
      "output time tensor([[nan]])\n",
      "input_cat tensor([0.0000,    nan,    nan], grad_fn=<CatBackward>)\n",
      "tensor([nan], grad_fn=<ThAddBackward>)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "loss = nn.NLLLoss()\n",
    "learning_rate = 0.0005\n",
    "rnn = Rmtpp(10)\n",
    "\n",
    "\n",
    "\n",
    "def train(time,marker):\n",
    "    #time and marker are list object\n",
    "    time = torch.tensor(time).unsqueeze(-1)\n",
    "    marker = torch.tensor(marker).unsqueeze(-1).float()\n",
    "    loss = 0\n",
    "    hidden = torch.zeros(1)\n",
    "    for j in range(len(time)):\n",
    "        tj = time[j]\n",
    "        print(j)\n",
    "        print(\"input time\",tj)\n",
    "        \n",
    "        yj = marker[j]\n",
    "        logprob, logfstar, hidden = rnn(tj, yj, hidden)\n",
    "        #print(\"hidden\",hidden)\n",
    "        print(\"-\"*10)\n",
    "\n",
    "\n",
    "time,marker = simulate_timestamps(5)\n",
    "train(time,marker)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tick.Hawkes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tick.plot import plot_point_process\n",
    "from tick.hawkes import SimuHawkes, HawkesKernelSumExp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 dimensional Hawkes process simulation using tick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x400 with 2 Axes>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_time = 40\n",
    "\n",
    "hawkes = SimuHawkes(n_nodes=1, end_time=run_time, verbose=False, seed=1398)\n",
    "kernel1 = HawkesKernelSumExp([.1, .2, .1], [1., 3., 7.])\n",
    "hawkes.set_kernel(0, 0, kernel1)\n",
    "hawkes.set_baseline(0, 1.)\n",
    "\n",
    "dt = 0.01\n",
    "hawkes.track_intensity(dt)\n",
    "hawkes.simulate()\n",
    "timestamps = hawkes.timestamps\n",
    "intensity = hawkes.tracked_intensity\n",
    "intensity_times = hawkes.intensity_tracked_times\n",
    "\n",
    "_, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
    "plot_point_process(hawkes, n_points=50000, t_min=0, max_jumps=20, ax=ax[0])\n",
    "plot_point_process(hawkes, n_points=50000, t_min=2, t_max=20, ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_timestamps(end_time):\n",
    "    # simulation 2 types of event for exemple selling or buying\n",
    "    \n",
    "    hawkes = SimuHawkes(n_nodes=2, end_time=end_time, verbose=False, seed=1398)\n",
    "    kernel = HawkesKernelSumExp([.1, .2, .1], [1., 3., 7.])\n",
    "    kernel1 = HawkesKernelSumExp([.2, .3, .1], [1., 3., 7.])\n",
    "    \n",
    "    hawkes.set_kernel(0, 0, kernel)\n",
    "    hawkes.set_kernel(0, 1, kernel)\n",
    "    hawkes.set_kernel(1, 0, kernel)\n",
    "    hawkes.set_kernel(1, 1, kernel)\n",
    "    \n",
    "    hawkes.set_baseline(0, .8)\n",
    "    hawkes.set_baseline(1, 1.)\n",
    "\n",
    "    dt = 0.1\n",
    "    hawkes.track_intensity(dt)\n",
    "    hawkes.simulate()\n",
    "    timestamps = hawkes.timestamps\n",
    "    t0 = timestamps[0]\n",
    "    t1 = timestamps[1]\n",
    "    \n",
    "    t = []\n",
    "    marker = []\n",
    "    n0 = len(t0)\n",
    "    n1 = len(t1)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while(i<n0 and j<n1):\n",
    "        if(t0[i]<t1[j]):\n",
    "            t.append(t0[i])\n",
    "            marker.append(0)\n",
    "            i += 1\n",
    "        else:\n",
    "            t.append(t1[j])\n",
    "            marker.append(1)\n",
    "            j += 1\n",
    "    if(i==n0):\n",
    "        for k in range(n0,n1):\n",
    "            t.append(t1[k])\n",
    "            marker.append(1)\n",
    "    else:\n",
    "        for k in range(n1,n0):\n",
    "            t.append(t0[k])\n",
    "            marker.append(0)\n",
    "        \n",
    "        \n",
    "   \n",
    "    return t,marker\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1453998273553042,\n",
       " 0.1841744724280548,\n",
       " 0.5910960339190892,\n",
       " 1.5677640924338307,\n",
       " 1.5824299252948109,\n",
       " 1.9706387517435,\n",
       " 2.930921868725349,\n",
       " 3.69765316580875,\n",
       " 3.828242832851533,\n",
       " 3.8690278220013226,\n",
       " 3.9924803129182216,\n",
       " 4.109933597350456,\n",
       " 4.2349385311236345,\n",
       " 4.528839214780729,\n",
       " 4.8825305589309345,\n",
       " 4.906513499400947,\n",
       " 4.973537632659001,\n",
       " 5.212763731186806,\n",
       " 5.331627287925645,\n",
       " 5.400715155736202,\n",
       " 5.406657026117649,\n",
       " 5.454771623071175,\n",
       " 5.456594504809564,\n",
       " 5.530331730185287,\n",
       " 5.578008222492138,\n",
       " 5.776994557096648,\n",
       " 5.819088834084263,\n",
       " 5.855813817764325,\n",
       " 5.8574804773489895,\n",
       " 5.860791449431528,\n",
       " 6.1324627680639185,\n",
       " 6.347360651207286,\n",
       " 6.972644024344269,\n",
       " 7.083367739295406,\n",
       " 7.210046934473388,\n",
       " 7.371146157026261,\n",
       " 7.4045067574341275,\n",
       " 7.4175271740402025,\n",
       " 7.459357874777104,\n",
       " 7.563767041208158,\n",
       " 7.606555100513292,\n",
       " 7.679454924040601,\n",
       " 7.773057141798235,\n",
       " 7.781373919771833,\n",
       " 7.813868599569604,\n",
       " 7.836330879915978,\n",
       " 7.84895889372964,\n",
       " 7.879810941526391,\n",
       " 7.881163676710684,\n",
       " 7.889762141105848,\n",
       " 7.9206452928985565,\n",
       " 7.949374006121685,\n",
       " 8.093230805709997,\n",
       " 8.11331293816528,\n",
       " 8.136138556997716,\n",
       " 8.187917443241997,\n",
       " 8.208997930140876,\n",
       " 8.247117948598683,\n",
       " 8.303120400569158,\n",
       " 8.33503331587932,\n",
       " 8.336986674110209,\n",
       " 8.46706594919943,\n",
       " 8.508160031466135,\n",
       " 8.76263219948387,\n",
       " 8.773827692469823,\n",
       " 8.796111868358015,\n",
       " 8.84409982834757,\n",
       " 8.925314481016407,\n",
       " 8.942837533656885,\n",
       " 9.036593558736861,\n",
       " 9.14337275323525,\n",
       " 9.155300255502798,\n",
       " 9.332115399760415,\n",
       " 9.341968262267331,\n",
       " 9.391756641297263,\n",
       " 9.626428884275965,\n",
       " 9.94428453025568,\n",
       " 10.20035026048413,\n",
       " 10.234776624536037,\n",
       " 10.491281053382497,\n",
       " 10.705363358938815,\n",
       " 10.856508572604348,\n",
       " 10.922340160368424,\n",
       " 10.933613031866475,\n",
       " 11.048747333732942,\n",
       " 11.11217655934394,\n",
       " 11.561000920764114,\n",
       " 11.593538404858682,\n",
       " 11.651830217259972,\n",
       " 11.70811285208783,\n",
       " 11.817102342318451,\n",
       " 11.885643459190254,\n",
       " 11.952212351905057,\n",
       " 12.015152120565086,\n",
       " 12.017833851917773,\n",
       " 12.019090329667316,\n",
       " 12.038405306540612,\n",
       " 12.054834147618102,\n",
       " 12.167526960773738,\n",
       " 12.205226871450348,\n",
       " 12.249870280217028,\n",
       " 12.255159838812324,\n",
       " 12.394676467591633,\n",
       " 12.459893350485142,\n",
       " 12.491395312771472,\n",
       " 12.512680166751496,\n",
       " 12.537044352839667,\n",
       " 12.570976155921365,\n",
       " 12.634555519619644,\n",
       " 12.708393532817729,\n",
       " 12.750804692072908,\n",
       " 12.933277502901188,\n",
       " 13.018982871294046,\n",
       " 13.078112437975623,\n",
       " 13.166000108184775,\n",
       " 13.229230099423997,\n",
       " 13.499332000804884,\n",
       " 13.568150519252793,\n",
       " 13.57609057619516,\n",
       " 13.661722299380784,\n",
       " 13.77822972229552,\n",
       " 13.797971052471846,\n",
       " 14.004848036446017,\n",
       " 14.058020599532052,\n",
       " 14.254974943793526,\n",
       " 14.410034600652672,\n",
       " 14.569836541928781,\n",
       " 14.645019289848026,\n",
       " 14.700247418998643,\n",
       " 14.888669742513638,\n",
       " 14.904306496137071,\n",
       " 14.919431938055087,\n",
       " 14.966586733705373,\n",
       " 14.993184974334454,\n",
       " 15.161436857139956,\n",
       " 15.182046975283994,\n",
       " 15.203803347476095,\n",
       " 15.216944384973479,\n",
       " 15.23811192588415,\n",
       " 15.252625450912614,\n",
       " 15.28833282679943,\n",
       " 15.288408451176016,\n",
       " 15.313937510643576,\n",
       " 15.354862396963894,\n",
       " 15.356566192151195,\n",
       " 15.423226575184076,\n",
       " 15.50864768429455,\n",
       " 15.51587017637451,\n",
       " 15.538628079608694,\n",
       " 15.588519847751284,\n",
       " 15.625841221389834,\n",
       " 15.800245026483587,\n",
       " 15.90894262228879,\n",
       " 15.924210297714772,\n",
       " 15.947581743318782,\n",
       " 16.057395889535453,\n",
       " 16.084923152102444,\n",
       " 16.56630324581187,\n",
       " 16.602157160863563,\n",
       " 16.76478809849576,\n",
       " 16.798188155840656,\n",
       " 17.02524321667147,\n",
       " 17.06777733243852,\n",
       " 17.06817689630501,\n",
       " 17.162773136409694,\n",
       " 17.49300084072248,\n",
       " 17.717081596016712,\n",
       " 17.824306605223708,\n",
       " 17.965115595505047,\n",
       " 18.348820963880243,\n",
       " 18.382313596069043,\n",
       " 18.456832048429455,\n",
       " 18.627661656209632,\n",
       " 18.768561002522144,\n",
       " 18.927345268752397,\n",
       " 19.009117820182922,\n",
       " 19.49014485198607,\n",
       " 19.49309054929352,\n",
       " 19.591051668653385,\n",
       " 19.662791753138865,\n",
       " 19.74264274690752,\n",
       " 19.75254571608325,\n",
       " 19.764107564878067,\n",
       " 19.7984555889917,\n",
       " 19.8036480237265]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate_timestamps(end_time=20)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
