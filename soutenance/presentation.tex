% !TeX spellcheck = fr_FR
\documentclass{beamer}
\setbeamercolor{background canvas}{bg=white}

\usetheme{metropolis}
\metroset{sectionpage=none,progressbar=frametitle}

\usepackage{appendixnumberbeamer}
\usepackage{polyglossia}
\setdefaultlanguage{french}
\usepackage{mathtools,amsfonts,amssymb}
\usepackage{booktabs}
\usepackage{enumitem}

\usepackage{subfiles}

\usepackage{csquotes}
\usepackage[
	backend=biber,
	sorting=nyt,
	style=alphabetic,
	maxcitenames=2,
	citestyle=authoryear
]{biblatex}
\addbibresource{../rapport/references.bib}

\usepackage{dsfont}
\usepackage{stmaryrd}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{siunitx}

\usepackage{hyperref}
\usepackage{cleveref}
\crefname{figure}{Figure}{Figures}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}

\DeclareMathOperator*{\argmax}{argmax} % no space, limits underneath in displays

\title{
	\textit{Processus ponctuels et réseaux de neurones récurrents}
}
\author{Cheikh Fall\\Wilson Jallet}

\date{11 décembre 2018}

\begin{document}
\maketitle

\begin{frame}{Introduction}
Beaucoup de phénomènes se manifestent par des événements ponctuels.\pause

L'arrivée de certains événements peut favoriser l'arrivée d'autres dans le futur.
\end{frame}

\subfile{models.tex}

\begin{frame}{Apprentissage}
Maximiser la fonction de log-vraisemblance
\begin{equation}\label{eq:logLikelihood}
\begin{aligned}
\ell &= P\left( \{(t_i,k_i)\}_i \mid \Theta \right) \\
&= \sum_{i=1}^{I}\log \lambda^{k_i}_{t_i} - \int_0^T \bar{\lambda}_t\,dt.
\end{aligned}
\end{equation}
avec $\bar{\lambda}_t = \sum_k \lambda^k_t$ l'intensité totale.

\textbf{Remarque} Difficile à implémenter! Calcul lourd.

$\rightarrow$ Entraînement sur les machines des salles info (avec GPU)
\end{frame}


\begin{frame}{Simulation de processus}
Simulation d'une trajectoire $\rightarrow$ version adaptée de l'algorithme d'\citeauthor{ogata1981} (\citeyear{ogata1981}).
\end{frame}

\begin{frame}{Prédiction}
On donne un début de séquence $(t_1,k_1)\ldots (t_{i-1}, k_{i-1})$ au réseau pour générer les paramètres pour $(t_{i-1}, t_i]$.
\begin{itemize}
	\item[$\rightarrow$] On peut calculer $\lambda_t = f(t\mid\mathcal{F}_{t_{i-1}})$ pour $t\geq t_{i-1}$
	\item[$\rightarrow$] Permet d'estimer le prochain événement $(t_i, k_i)$
	\[
	\begin{aligned}
		\hat{t}_i &= \EE\left[t_i \middle| \mathcal{F}_{t_{i-1}}\right] \\
		\hat{k}_i &= \argmax_{k}\EE\left[\lambda^k_t/\bar{\lambda}_t\middle| \mathcal{F}_{t_{i-1}} \right]
	\end{aligned}
	\]
\end{itemize}
\end{frame}

\section{Expériences}

\subfile{results.tex}

\section{Conclusion}

\begin{frame}{Conclusion}
Des modèles neuronaux peuvent fournir une bonne puissance de modélisation
\begin{itemize}
	\item[$\rightarrow$] faire un choix d'architecture 
	\item[\textcolor{blue}{$\rightarrow$}] différents comportements possibles
	\item[\textcolor{blue}{$\rightarrow$}] arrive à répliquer des Hawkes\pause
	\item[\textcolor{red}{$\rightarrow$}] implémentation délicate
	\item[\textcolor{red}{$\rightarrow$}] beaucoup de paramètres (gérer l'\textit{overfitting})
	\item[\textcolor{red}{$\rightarrow$}] échec sur les données réelles
\end{itemize}
\end{frame}

\begin{frame}{Pistes possibles}

\begin{itemize}
	\item[\textbullet] Gérer des processus marqués
	\item[\textbullet] \cite[42]{2015arXiv150204592B} propose de remplacer la log-vraisemblance $\ell$ par une fonction de contraste (perte $L_2$ sur l'intensité) $C = \EE[\|\lambda_\Theta - \tilde{\lambda} \|^2]$.
	\item[\textbullet] \citeauthor{xu2018poppy} propose une librairie de modélisation de processus ponctuels (en \texttt{PyTorch}), utilisant l'entropie $-\sum_i \log P(k_i\mid t_i,\mathcal{F}_{t_{i-1}})$. \cite{xu2018poppy,xuPatientFlow2017}
\end{itemize}



\end{frame}

\begin{frame}[t,allowframebreaks]{Bibliographie}
	\printbibliography
\end{frame}

\appendix

\begin{frame}{Expérience: Hawkes somme d'exponentielles}
On prend un Hawkes bivarié de noyau une somme d'exponentielles:
\[
	g_{ij}(t) = \sum_u \alpha_{ij}^u\beta_{ij}^u e^{-\beta^ut}\mathds{1}_{t > 0}
\]
\end{frame}

\begin{frame}
\begin{figure}
	\includegraphics[width=\linewidth]{../results/intensity_hawkesSumExp2D.pdf}
	\caption{Intensité du processus Hawkes à noyau somme d'exponentielles, généré par \texttt{tick}.}\label{fig:hawkesSumExpIntensityPlot}
\end{figure}
\end{frame}

\begin{frame}
\begin{figure}
	\includegraphics[width=\linewidth]{../results/intensity_HawkesLSTM_2d-sumexp_hidden128_20181210-004752.pdf}
	\caption{Trajectoire générée par un réseau LSTM entraîné sur le Hawkes \cref{fig:hawkesSumExpIntensityPlot}.}
\end{figure}
\end{frame}

\begin{frame}
\begin{figure}
	\includegraphics[width=\linewidth]{../results/2D_HawkesSumExp_Data_RMSE_20181210-021820.pdf}\caption{Résultats pour la prédiction.}
\end{figure}
\end{frame}
\end{document}
